\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[sort,compress]{natbib} % flexible bibliography support


\title{Project Proposal: \\Cluster Based Variance Reduction For Neural Nets}


\author{
Adithya Sagar\\
asg242@cornell.edu
\And
Yang Yuan  \\
Department of Computer Science\\
yy528@cornell.edu
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

% The following sections should be in the whitepaper
%a) motivation: what is the problem you are trying to solve / question to investigate
%b) approach: roughly what will be your approach (what will you do if this doesn?t work?)
%c) data: where does your data come from? Why is it the right data for this problem?
%d) evaluation: what experiments are you planning on evaluating your research / hypothesis.

\section{Proposal}
\subsection{Motivation}
Deep learning is now becoming a widely prevalent technique for solving various machine learning problems that include object recognition, speech recognition, auto driving, machine translation, etc. 
However, one of the major impediments about neural networks is their slow training time. Though certain degree of success has been achieved through the use of distributed machines with powerful GPUs to train deep neural nets, the the underlying training algorithms have only been few. They primarily include stochastic gradient descent (SGD), or SGD with auto tuning step size (e.g, AdaGrad and RMSProp), or adaptation of Nesterov's acceleration algorithm to the neural nets (e.g., Momentum and Adam). %Do we need more examples? Yang: I don't know others :(
%@yang - you could correct me here 
These algorithms are only a subset of what has been used by the optimization community. Thus training Deep Nets faster is a significant challenge which probably could be addressed by considering novel algorithms that have been developed in the area of optimization.  

%Yang: I think introduction of VR methods should be counted as motivation
Recently, a new category of algorithms called variance reduction (VR) methods have been proposed, \cite{JohnsonZhang2013-SVRG,Shalev-Shwartz2013-SDCA,Schmidt2013-SAG,Shalev-ShwartzZhang2014-ProxSDCA,XiaoZhang2014-ProximalSVRG,Defazio2014-Finito,Defazio2014-SAGA,Mairal2015-MISO,UniVR, exploitingstructure}, which greatly improved the convergence rate of the vanilla SGD by reducing the variance of each stochastic gradient
%Yang: I think here "each" is OK, since indeed every stochastic gradient has reduced variance after adjustment
during the training. The basic idea of variance reduction is to store some previous stochastic gradient information, and then use that to be adjustment for the current stochastic gradient, which helps reduce the variance.
However, most of the papers have considered convex or even strongly convex objectives, in which the proposed algorithms have strong theoretical guarantee. Several papers consider the case of a convex objective function that can be written as the sum of non-convex sub-functions (\cite{Shalev-Shwartz2015-SDCAwithoutDual,UniVR}), that can be seen a slight relaxation of convex objective. However this condition is not weak enough to provide any meaningful insights for highly non-convex objectives like neural nets. Recent work by Johnson et al.  \cite{JohnsonZhang2013-SVRG} presented a few preliminary results using this technique on feed-forward neural networks, where SVRG is better than SGD with best tuned step size in terms of test error and loss function for MNIST and CIFAR10. However, they have not presented an adequate discussion on other neural network architectures like convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Thus it will make an interesting study to quantify the performance of VR based optimization method on different neural network architectures. 

\subsection{Approach}

%We need to described a little bit more in detail about what the VR method is - write the primary equation
\newcommand{\tnabla}{\tilde{\nabla}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\bx}{\bar{x}}
\newcommand{\lt}{\mathsf{lt}}
\newcommand{\li}{\mathsf{li}}
\newcommand{\cH}{H_{\mathsf{cl}}}

Recall that in a stochastic gradient method one usually defines a gradient estimator $\tnabla^t$ at iteration $t$, and performs an update $x^{t+1} \gets x^t - \eta \tnabla^t$ for some step size $\eta>0$.
The key idea behind most VR methods is to carefully define an estimator $\tnabla^t$ with diminishing variance. For instance, SVRG chooses $\tnabla^t = \frac{1}{n} \sum_j \nabla f_j(\tx) + \nabla f_i(x^t) - \nabla f_i(\tx)$ where $\tx$ is a \emph{snapshot} iterate that can be $n$ iterations ago.

In addition, VR methods have exploited ~\cite{exploitingstructure,HLM2015} the internal structure of the data using different clustering techniques like ClusterSVRG and ClusterACDM~\cite{exploitingstructure}, to enhance the speed of optimization. Cluster dependent VRs will be much faster than their vanilla version (SVRG and ACDM), based on the quality of the structure.   
%Would this be needed? - how do we cluster? 
%Yang: we should use some clustering algorithm to get the cluster information of the data, for example, LSH. We need to mention that clustering process, as the preprocess step, is fast; otherwise it's non-sense to use the clustering idea..
Moreover, the required clustering on the data set can be raw and can contain a few outliers, and can be computed very efficiently in just one pass of the data, using standard nearest neighbor algorithm like LSH. 

This idea could be particularly interesting for deep learning applications, since it is common practice to create augment data set using trivial transformation or adding small perturbation on the existing training data, in order to get better training results. Based on the definition of clustering structure~\cite{exploitingstructure}, most of the augmented data points can be seen a inside the same cluster as the original data point, which shows clear advantage of the newly proposed algorithms. Moreover, even without the augment data set, it is natural to assume good clustering structure inside the original data set. For example, one person's face seen from slightly different angles can be treated as from the same cluster, and certain animals from the same category could form one cluster as well. 


%Yang: maybe below is a minor point and we don't really need to mention it..
%One issue that we have in mind is that since variance reduction method reduces the variance of the stochastic gradient, it might be more susceptible to stuck at saddle points. According to \cite{escapesaddle}, one could add a uniform noise in first-order optimization methods for escaping non-degenerate saddle points. We will also try this idea if necessary.

\subsection{Data}
We will first try small data sets like MNIST, CIFAR10, CIFAR100. Since several SGD based optimization algorithms  have traditionally used these data sets, it will be interesting to compare the performance of our approach on these data sets. In the event that we are performing well on these data sets we will attempt to use them on larger more complex data sets like MIMIC III \cite{mimiciii} and data sets that describe traumatic coagulopathy.

\subsection{Evaluation}
We would like to try to apply VR methods or cluster based VR method to explore how neural nets will benefit from that. We are planning to try different neural nets, like feed-forward neural nets, convolutional neural nets, and also recurrent neural nets, with different structure designs, and different types of data set. The final outcome of this project will be a detailed analysis and comparison of the performance of cluster based VR method in different scenarios. We will investigate different dimensions of performance including convergence of objective function and training error, training time, etc. 


%Adi: you can add something more of surrogate functions.
%^^ I added a little bit about surrogates in the approach section


\bibliographystyle{plain}
\bibliography{ml}
\end{document}
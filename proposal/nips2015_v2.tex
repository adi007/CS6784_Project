\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[sort,compress]{natbib} % flexible bibliography support


\title{Project Proposal: \\Cluster Based Variance Reduction For Neural Nets}


\author{
Gideon Dresdner\\
Department of Computational Biology\\
gmd87@cornell.edu
\And
Adithya Sagar\\
asg242@cornell.edu
\And
Yang Yuan  \\
Department of Computer Science\\
yy528@cornell.edu
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

% The following sections should be in the whitepaper
%a) motivation: what is the problem you are trying to solve / question to investigate
%b) approach: roughly what will be your approach (what will you do if this doesn?t work?)
%c) data: where does your data come from? Why is it the right data for this problem?
%d) evaluation: what experiments are you planning on evaluating your research / hypothesis.

\section{Proposal}
\subsection{Motivation}
Deep learning is becoming a widely prevalent technique for solving various machine learning problems such as object recognition, speech recognition, automobile driving and machine translation. 
Though slow training time continues to be a major impediment to using neural network models,
%Though many successes have been achieved by leveraging the power of large distributed systems equipped with GPUs to train deep neural nets, there have been few advances in the training algorithms that underpin neural network models. These training algorithms are primarily either stochastic gradient descent (SGD), SGD with auto tuning step size (e.g, AdaGrad and RMSProp) or adaptations of Nesterov's acceleration algorithm to the neural nets (e.g., Momentum and Adam). %Do we need more examples? Yang: I don't know others :(
%@yang - you could correct me here 
surprisingly few algorithmic developments made in the optimization community have been applied to neural networks. Thus there is an opportunity to bring novel algorithms from the optimization community to deep neural networks.
Variance reduction (VR) is one such category of algorithms from the optimization community.
These techniques greatly improve the convergence rate of vanilla SGD by reducing the variance of the stochastic gradients computed during training. \cite{JohnsonZhang2013-SVRG,Shalev-Shwartz2013-SDCA,Schmidt2013-SAG,Shalev-ShwartzZhang2014-ProxSDCA,XiaoZhang2014-ProximalSVRG,Defazio2014-Finito,Defazio2014-SAGA,Mairal2015-MISO,UniVR, exploitingstructure}.
%The basic idea of variance reduction is to store stochastic gradient information from a previous iteration in order to reduce the variance of some future iterations.
%Yang: I think here "each" is OK, since indeed every stochastic gradient has reduced variance after adjustment
%Yang: I think introduction of VR methods should be counted as motivation

% However, most of the papers have considered convex or even strongly convex objectives, in which the proposed algorithms have strong theoretical guarantee. Several papers consider the case of a convex objective function that can be written as the sum of non-convex sub-functions (\cite{Shalev-Shwartz2015-SDCAwithoutDual,UniVR}), that can be seen a slight relaxation of convex objective. However this condition is not weak enough to provide any meaningful insights for highly non-convex objectives like neural nets. Recent work by Johnson et al.  \cite{JohnsonZhang2013-SVRG} presented a few preliminary results using this technique on feed-forward neural networks, where SVRG is better than SGD with best tuned step size in terms of test error and loss function for MNIST and CIFAR10. However, they have not presented an adequate discussion on other neural network architectures like convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Thus it will make an interesting study to quantify the performance of VR based optimization method on different neural network architectures. 

\subsection{Approach}

%We need to described a little bit more in detail about what the VR method is - write the primary equation
\newcommand{\tnabla}{\tilde{\nabla}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\bx}{\bar{x}}
\newcommand{\lt}{\mathsf{lt}}
\newcommand{\li}{\mathsf{li}}
\newcommand{\cH}{H_{\mathsf{cl}}}

The basic idea of variance reduction is to store stochastic gradient information from a previous iteration in order to reduce the variance of some future iterations. Recall that in a stochastic gradient method one usually defines a gradient estimator $\tnabla^t$ at iteration $t$, and performs an update $x^{t+1} \gets x^t - \eta \tnabla^t$ for some step size $\eta>0$.
The key idea behind most VR methods is to carefully define an estimator $\tnabla^t$ with diminishing variance. For instance, SVRG chooses $\tnabla^t = \frac{1}{n} \sum_j \nabla f_j(\tx) + \nabla f_i(x^t) - \nabla f_i(\tx)$ where $\tx$ is a \emph{snapshot} iterate that can be $n$ iterations ago.

In addition, VR methods have exploited ~\cite{exploitingstructure,HLM2015} the internal structure of the data using different clustering techniques like ClusterSVRG and ClusterACDM~\cite{exploitingstructure}, to enhance the speed of optimization. Cluster dependent VRs will be much faster than their vanilla version (SVRG and ACDM), based on the quality of the structure.   
%Would this be needed? - how do we cluster? 
%Yang: we should use some clustering algorithm to get the cluster information of the data, for example, LSH. We need to mention that clustering process, as the preprocess step, is fast; otherwise it's non-sense to use the clustering idea..
Moreover, the required clustering on the data set can be raw and can contain a few outliers, and can be computed very efficiently in just one pass of the data, using standard nearest neighbor algorithm like LSH. 

However, VR has almost exclusively only been applied to convex objectives of various forms. Only recently has Johnson et al. begun to use VR to train feed-forward neural networks \cite{JohnsonZhang2013-SVRG} leaving the field wide open for the study of VR in CNNs or RNNs.

% However, most of the papers have considered convex or even strongly convex objectives, in which the proposed algorithms have strong theoretical guarantee. Several papers consider the case of a convex objective function that can be written as the sum of non-convex sub-functions (\cite{Shalev-Shwartz2015-SDCAwithoutDual,UniVR}), that can be seen a slight relaxation of convex objective. However this condition is not weak enough to provide any meaningful insights for highly non-convex objectives like neural nets. Recent work by Johnson et al.  \cite{JohnsonZhang2013-SVRG} presented a few preliminary results using this technique on feed-forward neural networks, where SVRG is better than SGD with best tuned step size in terms of test error and loss function for MNIST and CIFAR10. However, they have not presented an adequate discussion on other neural network architectures like convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Thus it will make an interesting study to quantify the performance of VR based optimization method on different neural network architectures. 

% This idea could be particularly interesting for deep learning applications, since it is common practice to create augment data set using trivial transformation or adding small perturbation on the existing training data, in order to get better training results. Based on the definition of clustering structure~\cite{exploitingstructure}, most of the augmented data points can be seen a inside the same cluster as the original data point, which shows clear advantage of the newly proposed algorithms. Moreover, even without the augment data set, it is natural to assume good clustering structure inside the original data set. For example, one person's face seen from slightly different angles can be treated as from the same cluster, and certain animals from the same category could form one cluster as well. 


%Yang: maybe below is a minor point and we don't really need to mention it..
%One issue that we have in mind is that since variance reduction method reduces the variance of the stochastic gradient, it might be more susceptible to stuck at saddle points. According to \cite{escapesaddle}, one could add a uniform noise in first-order optimization methods for escaping non-degenerate saddle points. We will also try this idea if necessary.

\subsection{Data}
Since we are experimenting with new training methods for neural networks, we will start by ``varying the algorithm while holding the data constant.'' MNIST, CIFAR-10 and CIFAR-100 are the best way for us to prove the exceptional performance of VR training. Ideally, we would like to move beyond these canonical datasets to clinical datasets like MIMIC III \cite{mimiciii} datasets related to traumatic coagulopathy.

\subsection{Evaluation}
Fortunately, neural network training performance is a well-defined problem and is relatively straight-forward to evaluate in terms of number of epochs, training error and test error. We can use these metrics to evaluate VR optimization algorithms compared with the current state of the art in neural network training.

%Adi: you can add something more of surrogate functions.
%^^ I added a little bit about surrogates in the approach section

\bibliographystyle{plain}
\bibliography{ml}
\end{document}
